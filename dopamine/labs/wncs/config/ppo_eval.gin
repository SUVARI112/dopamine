# Mujoco Hyperparameters following those specified in Table 3 of Appendix A in:
#   "Proximal Policy Optimization Algorithms"
#   by John Schulman et al.
#   https://arxiv.org/abs/1707.06347

Environment.include_zeros = True
Environment.cost_type = "log-cost" 
Environment.algorithm = "ppo"
Environment.setup = "evaluation"

# Hyperparameters for PPOAgent
PPOAgent.num_layers = 1
PPOAgent.hidden_units = 512
PPOAgent.activation = 'tanh'

update_period = 200
batch_size = 50 

PPOAgent.update_period = %update_period
PPOAgent.optimizer = 'adam'
PPOAgent.max_gradient_norm = 0.5

create_optimizer.anneal_learning_rate=True
create_optimizer.initial_learning_rate= 1e-4
create_optimizer.final_learning_rate=1e-5
create_optimizer.anneal_steps=50_000
create_optimizer.eps = 1e-5

PPOAgent.num_epochs = 10
PPOAgent.batch_size = %batch_size
PPOAgent.gamma = 0.99
PPOAgent.lambda_ = 0.95
PPOAgent.epsilon = 0.2
PPOAgent.vf_coefficient = 0.5
PPOAgent.entropy_coefficient = 0.01
PPOAgent.clip_critic_loss = True
PPOAgent.seed = None  # Seed with the current time

create_runner.schedule = 'eval'
create_agent.agent_name = 'ppo'
create_agent.debug_mode = True
EvalRunner.num_iterations = 10
EvalRunner.evaluation_steps = 500
EvalRunner.max_steps_per_episode = 500

ReplayBuffer.max_capacity = %update_period
ReplayBuffer.batch_size = %batch_size
