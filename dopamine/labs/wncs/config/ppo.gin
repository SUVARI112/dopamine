# Mujoco Hyperparameters following those specified in Table 3 of Appendix A in:
#   "Proximal Policy Optimization Algorithms"
#   by John Schulman et al.
#   https://arxiv.org/abs/1707.06347

PPOAgent.num_layers = 2
PPOAgent.hidden_units = 64
PPOAgent.activation = 'tanh'
PPOAgent.update_period = 20
PPOAgent.optimizer = 'adam'
PPOAgent.max_gradient_norm = 0.5

create_optimizer.anneal_learning_rate=False
create_optimizer.initial_learning_rate= 3e-4
create_optimizer.final_learning_rate=3e-5
create_optimizer.anneal_steps=50_000
create_optimizer.eps = 1e-5

PPOAgent.num_epochs = 10
PPOAgent.batch_size = 10
PPOAgent.gamma = 0.99
PPOAgent.lambda_ = 0.95
PPOAgent.epsilon = 0.2
PPOAgent.vf_coefficient = 0.5
PPOAgent.entropy_coefficient = 0.01
PPOAgent.clip_critic_loss = True
PPOAgent.seed = None  # Seed with the current time


Environment.include_zeros = True
Environment.cost_type = "log-cost" 
Environment.algorithm="ppo"

create_runner.schedule = 'continuous_train'
create_agent.agent_name = 'ppo'
create_agent.debug_mode = True
Runner.num_iterations = 1
Runner.training_steps = 250_000
Runner.evaluation_steps = 500
Runner.max_steps_per_episode = 500
Runner.clip_rewards=False

ReplayBuffer.max_capacity = 20
ReplayBuffer.batch_size = 20
