# Mujoco Hyperparameters following those specified in Table 3 of Appendix A in:
#   "Proximal Policy Optimization Algorithms"
#   by John Schulman et al.
#   https://arxiv.org/abs/1707.06347

Environment.include_zeros = False
Environment.cost_type = "log-cost" 
Environment.algorithm="ppo"
Environment.aoi_threshold=30
Environment.terminal_cost=10000
# Environment.setup= "evaluation"

# Hyperparameters for PPOAgent
PPOAgent.num_layers = 1
PPOAgent.hidden_units = 512
PPOAgent.activation = 'tanh'

update_period = 150
batch_size = 50 

PPOAgent.update_period = %update_period
PPOAgent.optimizer = 'adam'
PPOAgent.max_gradient_norm = 0.5

create_optimizer.anneal_learning_rate=True
create_optimizer.initial_learning_rate= 3e-5
create_optimizer.final_learning_rate=1e-5
create_optimizer.anneal_steps=100_000
create_optimizer.eps = 1e-5

PPOAgent.num_epochs = 10
PPOAgent.batch_size = %batch_size
PPOAgent.gamma = 0.99
PPOAgent.lambda_ = 0.95
PPOAgent.epsilon = 0.2
PPOAgent.vf_coefficient = 0.5
PPOAgent.entropy_coefficient = 0.01
PPOAgent.clip_critic_loss = True
PPOAgent.seed = None  # Seed with the current time

create_runner.schedule = 'continuous_train'
create_agent.agent_name = 'ppo'
create_agent.debug_mode = True
Runner.num_iterations = 1
Runner.training_steps = 1_000_000
Runner.max_steps_per_episode = 500
Runner.clip_rewards=False

ReplayBuffer.max_capacity = %update_period
ReplayBuffer.batch_size = %batch_size
