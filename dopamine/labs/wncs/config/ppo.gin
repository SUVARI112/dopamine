# Mujoco Hyperparameters following those specified in Table 3 of Appendix A in:
#   "Proximal Policy Optimization Algorithms"
#   by John Schulman et al.
#   https://arxiv.org/abs/1707.06347

# PPOAgent.num_layers = 2
# PPOAgent.hidden_units = 64
# PPOAgent.activation = 'tanh'

# Hyperparameters for PPOAgent
PPOAgent.num_layers = 2
PPOAgent.hidden_units = 256
PPOAgent.activation = 'tanh'

PPOAgent.update_period = 100
PPOAgent.optimizer = 'adam'
PPOAgent.max_gradient_norm = 0.5

# create_optimizer.learning_rate = 3e-4
# create_optimizer.eps = 1e-5
# create_optimizer.anneal_learning_rate = True
# create_optimizer.anneal_steps = 160_000  # 500 iterations * 10 epochs * 2048 timesteps / 64 batches

create_optimizer.anneal_learning_rate=True
create_optimizer.initial_learning_rate= 3e-4
create_optimizer.final_learning_rate=3e-5
create_optimizer.anneal_steps=50_000
create_optimizer.eps = 1e-5

PPOAgent.num_epochs = 10
PPOAgent.batch_size = 25
PPOAgent.gamma = 0.99
PPOAgent.lambda_ = 0.95
PPOAgent.epsilon = 0.2
PPOAgent.vf_coefficient = 0.5
PPOAgent.entropy_coefficient = 0.01
PPOAgent.clip_critic_loss = True
PPOAgent.seed = None  # Seed with the current time


Environment.include_zeros = True
Environment.cost_type = "log-cost" 
Environment.algorithm="ppo"

# create_gym_environment.use_ppo_preprocessing = True
# create_continuous_runner.schedule = 'continuous_train'
# create_continuous_agent.agent_name = 'ppo'
# ContinuousTrainRunner.create_environment_fn = @gym_lib.create_gym_environment
# ContinuousRunner.num_iterations = 500
# ContinuousRunner.training_steps = 2048
# ContinuousRunner.max_steps_per_episode = None
# Runner.clip_rewards
create_runner.schedule = 'continuous_train'
create_agent.agent_name = 'ppo'
create_agent.debug_mode = True
Runner.num_iterations = 1
Runner.training_steps = 250_000
Runner.evaluation_steps = 500
Runner.max_steps_per_episode = 500
Runner.clip_rewards=False

ReplayBuffer.max_capacity = 20
ReplayBuffer.batch_size = 20
